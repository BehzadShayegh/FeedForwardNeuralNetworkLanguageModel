{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-vQzPI0PNERc",
    "outputId": "bc9fa401-7db4-48a8-b200-1bf31e22af7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.38.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pytorch-nlp\n",
    "from torchnlp.word_to_vector import GloVe\n",
    "g_vec = GloVe(name='6B', dim='50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1RWSYfs2QxI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "g_vocab = g_vec.token_to_index.keys()\n",
    "g_vocab = np.array(list(g_vocab))\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def retokenize(t) :\n",
    "  if type(t) == type(\"str\") :\n",
    "    tt = t\n",
    "  elif type(t) == type(list()) :\n",
    "    tt = ' '.join(t)\n",
    "  elif type(t) == type(np.array(list())) :\n",
    "    tt = ' '.join(t.tolist())\n",
    "  else :\n",
    "    return False\n",
    "  return np.array(nltk.tokenize.word_tokenize(tt))\n",
    "\n",
    "def researcher(word) :\n",
    "  i = 0\n",
    "  right = True\n",
    "  for i in range(len(word)) :\n",
    "    if word[i:] in g_vocab :\n",
    "      break\n",
    "    if word[:-i] in g_vocab :\n",
    "      right = False\n",
    "      break\n",
    "  new_word = word[i:] if right else word[:-i]\n",
    "  if len(new_word) < 3 :\n",
    "    new_word = '<UNK>'\n",
    "  return new_word\n",
    "  \n",
    "\n",
    "def clean_text(name) :\n",
    "  t = open('./'+name+'.txt').read()\n",
    "  t = t.lower()\n",
    "  t = retokenize(t)\n",
    "  not_found = ~np.isin(t, g_vocab)\n",
    "  t[not_found] = list(map(lambda w: re.sub('//\\S*', 'url', w), tqdm(t[not_found])))\n",
    "  not_found = ~np.isin(t, g_vocab)\n",
    "  t[not_found] = list(map(lambda w: re.sub(\"(\\d+)\", ' \\1 ', w), tqdm(t[not_found])))\n",
    "  t = retokenize(t)\n",
    "  not_found = ~np.isin(t, g_vocab)\n",
    "  t[not_found] = list(map(lambda w: re.sub(\"[^a-zA-Z0-9|\\-|']\", ' ', w), tqdm(t[not_found])))\n",
    "  t = retokenize(t)\n",
    "  not_found = ~np.isin(t, g_vocab)\n",
    "  t[not_found] = list(map(lambda w: re.sub(\"[\\-|']\", ' ', w), tqdm(t[not_found])))\n",
    "  t = retokenize(t)\n",
    "  not_found = ~np.isin(t, g_vocab)\n",
    "  t[not_found] = list(map(stemmer.stem, tqdm(t[not_found])))\n",
    "  not_found = ~np.isin(t, g_vocab)\n",
    "  t[not_found] = list(map(lemmatizer.lemmatize, tqdm(t[not_found])))\n",
    "  t = retokenize(t)\n",
    "  not_found = ~np.isin(t, g_vocab)\n",
    "  t[not_found] = list(map(researcher, tqdm(t[not_found])))\n",
    "  t = t[t != '']\n",
    "  return t.tolist()\n",
    "\n",
    "train_text = clean_text('train')\n",
    "test_text = clean_text('test')\n",
    "\n",
    "import json\n",
    "\n",
    "json.dump(train_text, open('clean_train.json','w'))\n",
    "json.dump(test_text, open('clean_test.json','w'))\n",
    "\n",
    "vocab = np.unique(train_text+test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rUlnxdY9Thr"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "vocabl = vocab.tolist()\n",
    "i_train_text = list(map(vocabl.index, tqdm(train_text)))\n",
    "i_test_text = list(map(vocabl.index, tqdm(test_text)))\n",
    "\n",
    "json.dump(i_train_text, open('i_train.json','w'))\n",
    "json.dump(i_test_text, open('i_test.json','w'))\n",
    "del vocabl"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP-CA#3-Part0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
